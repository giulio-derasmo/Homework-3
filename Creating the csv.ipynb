{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0573ec-0d65-4e6f-881f-330230a4a72b",
   "metadata": {},
   "source": [
    "# Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3872f12-bc4e-4665-9eb9-4d91b1c2858a",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77437212-a96d-4b40-9ad7-ab400fba6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5b94c-d350-474e-8ac1-85d08788f1f2",
   "metadata": {},
   "source": [
    "## Gather Information [ I Try with one anime ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f45413d-77c1-4d11-b7b9-88916648a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_name = r\"./Folder_with_page/page1/anime_1.html\"\n",
    "with open(html_name, \"r\",  encoding='utf-8') as fp:\n",
    "    soup = BeautifulSoup(fp, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "15a46504-a45e-49c8-ac4c-e2fe83aec4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = [\"animeTitle\", \"animeType\", \"animeNumEpisode\",\"releaseDate\",\"endDate\"]\n",
    "list_of_anime = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9dc1574-8edb-426b-9c9c-778f5f6c8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_info = []\n",
    "# Anime Title\n",
    "anime_info.append(soup.find(\"h1\", attrs = {\"class\": \"title-name h1_bold_none\"}).string)\n",
    "\n",
    "# Taken Information\n",
    "for h2 in soup.select('h2:has(+div)'):\n",
    "    # I want the <h2> Information </h2>\n",
    "    if h2.text == \"Information\" :\n",
    "        # iter over the next 4 <div> information\n",
    "        for inform in h2.find_all_next(\"div\", attrs = {\"class\": \"spaceit_pad\"}, limit = 4):\n",
    "            if inform.contents[1].string == \"Type:\":\n",
    "                anime_info.append( inform.get_text(separator=\" \", strip=True).split()[-1] )\n",
    "            if inform.contents[1].string == \"Episodes:\":\n",
    "                anime_info.append( inform.get_text(separator=\" \", strip=True).split()[-1] )\n",
    "            if inform.contents[1].string == \"Aired:\":\n",
    "                anime_info.append( inform.get_text(separator=\" \", strip=True).split()[1:4] )\n",
    "                anime_info.append( inform.get_text(separator=\" \", strip=True).split()[5:8] )\n",
    "            \n",
    "# scrab of the anime finish\n",
    "list_of_anime.append(anime_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "60cb9d64-98bb-48c2-bbb0-1ce37adc285f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fullmetal Alchemist: Brotherhood',\n",
       "  'TV',\n",
       "  '64',\n",
       "  ['Apr', '5,', '2009'],\n",
       "  ['Jul', '4,', '2010']]]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_anime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5eb0f9-1876-43c4-a734-50d9c36e2021",
   "metadata": {},
   "source": [
    "# Iterating over the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8619b0f4-e930-4d7b-9345-d90cdfaea382",
   "metadata": {},
   "source": [
    "Define a function to scrab the anime (incomplite)\n",
    "\n",
    "* anime_info: is a list that store the information of the anime in order of attributes list (the column of the dataframe)\n",
    "\n",
    "Than the anime_info is store inside the __list_of_anime__ a list that store all the anime_info list. Than is pass to a daframe and create the tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5bb47f3a-3c48-4f23-9681-ff10f5d5c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(date_list):\n",
    "    # cheack the date is store as [Month, Day, Year]\n",
    "    # there is date store as [empty] if film (end date), [?] if still airing\n",
    "    if len(date_list) == 3:\n",
    "        date_string = \" \".join(date_list).replace(',', '')\n",
    "        # %b = for abbreviate month\n",
    "        new_date = datetime.strptime(date_string, '%b %d %Y')\n",
    "    else:\n",
    "        # return empty\n",
    "        new_date = None\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d406cf0-2901-4a40-88f5-901bce364076",
   "metadata": {},
   "source": [
    "The function scrabbing_anime takes the __INFORMATION__ info of the anime + the __TITLE__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "651c62b5-9ec7-43ba-8c04-f92bf5ecbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrabbing_anime(soup):\n",
    "    anime_info = []\n",
    "    # Anime Title\n",
    "    # <h1 class=\"title-name h1_bold_none\"><strong>Fullmetal Alchemist: Brotherhood</strong></h1>\n",
    "    anime_info.append(str(soup.find(\"h1\", attrs = {\"class\": \"title-name h1_bold_none\"}).string))\n",
    "    # Taken Information\n",
    "    # <h2>Information</h2> , there are a lot of h2 so i specify is written >>inside a div<<\n",
    "    for h2 in soup.select('h2:has(+div)'):\n",
    "        # I want the <h2> Information </h2> only\n",
    "        if h2.text == \"Information\" :\n",
    "            # iter over the next 4 <div> of Information, i go this way because\n",
    "            # 1) i want to skip the \"Status\", is the 3th div\n",
    "            # 2) more clear\n",
    "            for inform in h2.find_all_next(\"div\", attrs = {\"class\": \"spaceit_pad\"}, limit = 4):\n",
    "                if inform.contents[1].string == \"Type:\":\n",
    "                    anime_info.append( inform.get_text(separator=\" \", strip=True).split()[-1] )\n",
    "                if inform.contents[1].string == \"Episodes:\":\n",
    "                    anime_info.append(int(inform.get_text(separator=\" \", strip=True).split()[-1]) )\n",
    "                if inform.contents[1].string == \"Aired:\":\n",
    "                    anime_info.append( parse_time(inform.get_text(separator=\" \", strip=True).split()[1:4] ))\n",
    "                    anime_info.append( parse_time(inform.get_text(separator=\" \", strip=True).split()[5:8] ))\n",
    "                    \n",
    "    # The scrabbing is finish, return the anime list with the info in order\n",
    "    return anime_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "910a57fe-38d2-49f9-a703-99b872039009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrabbing:\n",
    "# Score, Ranked, Poplarity, Members\n",
    "def scrabbing_anime2(soup, anime_info):\n",
    "    # score\n",
    "    score = soup.find(\"div\", attrs = {\"class\": \"fl-l score\", \"data-title\": \"score\"})\n",
    "    anime_info.append(float(score.contents[0].string))\n",
    "    # I get in to the block of information I need\n",
    "    tag = soup.find(\"div\", attrs = {\"class\": \"di-ib ml12 pl20 pt8\"})\n",
    "    # they are all store inside a <strong> tag + cleaning the data\n",
    "    for strong_tag in tag.find_all('strong'):\n",
    "        string = strong_tag.string.replace(\",\",\"\")\n",
    "        # elimite \"#\"\n",
    "        s = int(re.sub(r\"#\", ' ', string))\n",
    "        anime_info.append(int(s))\n",
    "    return anime_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3efb4819-0b4c-4c86-9d1f-a51558a4bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "attrs = [\"animeTitle\", \"animeType\", \"animeNumEpisode\",\"releaseDate\",\"endDate\",\"animeScore\",\"animeRank\",\"animePopularity\",\"animeNumMembers\"]\n",
    "list_of_anime = []\n",
    "\n",
    "# from page 1 to 130\n",
    "# I tried only page 1\n",
    "for page in tqdm(range(1,2)):\n",
    "    folder = \"./Folder_with_page/page\"+str(page)\n",
    "    for anime in os.listdir(folder):\n",
    "        with open(folder + \"/\" + anime, \"r\",  encoding='utf-8') as fp:\n",
    "            soup = BeautifulSoup(fp, \"html.parser\")\n",
    "        anime_info = scrabbing_anime(soup)\n",
    "        anime_info + scrabbing_anime2(soup, anime_info)\n",
    "        list_of_anime.append(anime_info)\n",
    "        \n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(list_of_anime, columns = attrs)\n",
    "# Creating the tsv file\n",
    "df.to_csv('anime.tsv', index = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c870c753-c6b8-4d53-af54-b4f73385bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   animeTitle       50 non-null     object        \n",
      " 1   animeType        50 non-null     object        \n",
      " 2   animeNumEpisode  50 non-null     int64         \n",
      " 3   releaseDate      50 non-null     datetime64[ns]\n",
      " 4   endDate          38 non-null     datetime64[ns]\n",
      " 5   animeScore       50 non-null     float64       \n",
      " 6   animeRank        50 non-null     int64         \n",
      " 7   animePopularity  50 non-null     int64         \n",
      " 8   animeNumMembers  50 non-null     int64         \n",
      "dtypes: datetime64[ns](2), float64(1), int64(4), object(2)\n",
      "memory usage: 3.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# check the type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c0552-eb10-4b3b-b086-a3027817ce43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
