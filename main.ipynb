{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002970b4-b5da-48e8-b909-addc7ffaa346",
   "metadata": {},
   "source": [
    "# [EX1] Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d703a-9fe3-498e-aa3d-93add558fe50",
   "metadata": {},
   "source": [
    "## 1.1 Get the list of animes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d728227-696a-4fd9-8c0a-3bd1448f0bab",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8144771a-b1b0-4518-a549-8150b9da5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# for better time view \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa69a81-0e20-4e4d-9a6f-607a6ec6835b",
   "metadata": {},
   "source": [
    "### Creating the anime_url.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f37107-25fe-4988-9206-b8bc9abd77df",
   "metadata": {},
   "source": [
    "We are creating a file called \"anime.txt\" conteining the url of each anime in the 400 pages (actually 383) of the top anime rank list of the MyAnimeList site. \n",
    "\n",
    "* We initialize a filename and then we are doing a for loop over the 400 pages. If the url \"exist\" so the response == 200 and we can collect the 50 url in that page, otherwise if the response == 400, the page doesn't exist and we exit from the loop, no more interest in continue to loop (no anime url to collect).\n",
    "\n",
    "* The url scrabbing consist on using BeautifulSoup to inspect the whole html page to find where the url of the anime are. In particolar under the < tr>..< /tr> and < a>..< /a> we can view all the link in the pages. Moreover the anime url are associated with an \"id=#areaXXX\" class. We get there and gather the right link.\n",
    "```\n",
    "<a href=\"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\" id=\"#area5114\" rel=\"#info5114\">Fullmetal Alchemist: Brotherhood</a>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea6a0e02-bac0-434a-b27e-fca792ce96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████▌   | 383/400 [05:02<00:13,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End with page count:  383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = r\"./anime_url.txt\"\n",
    "with open(filename,'w', encoding='utf-8') as f:\n",
    "    for page in tqdm(range(0, 400)):\n",
    "        url = \"https://myanimelist.net/topanime.php?limit=\"+str(page*50)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            for tag in soup.find_all(\"tr\"):\n",
    "                links = tag.find_all(\"a\")\n",
    "                for link in links:\n",
    "                    if type(link.get(\"id\")) == str and len(link.contents[0]) > 1:\n",
    "                        f.write(link.get(\"href\"))\n",
    "                        f.write(\"\\n\")\n",
    "        else:\n",
    "            print(\"End with page count: \", page)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82006335-c135-4052-96cd-1e6deabef3e6",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa84d01-ce62-4dc5-b63a-e095abb2409e",
   "metadata": {},
   "source": [
    "We now need to collect for each anime the html and store it in a .html file. The anime needs to follow the ranking page of the MyAnimeList, i.e 50 anime for page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa470b1c-7472-41e9-aaa0-18ba68b1c3e1",
   "metadata": {},
   "source": [
    "For this we begin to create a list called \"lines\" that store all the link of the anime. We use this method instead of open directly the .txt file in for loop to split the crawl part among the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae37c5a8-d4cc-4dd3-b9a4-3ff596e692bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r\"./anime_url.txt\"\n",
    "\n",
    "lines = []\n",
    "with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a080211e-4055-4506-b454-58d11ac8a9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\\n',\n",
       "  'https://myanimelist.net/anime/28977/Gintama°\\n',\n",
       "  'https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2\\n',\n",
       "  'https://myanimelist.net/anime/9253/Steins_Gate\\n',\n",
       "  'https://myanimelist.net/anime/42938/Fruits_Basket__The_Final\\n'],\n",
       " ['https://myanimelist.net/anime/42383/Konbini_Shoujo_Z\\n',\n",
       "  'https://myanimelist.net/anime/10564/Korogashi_Ryouta\\n',\n",
       "  'https://myanimelist.net/anime/50237/Kyonyuu_Elf_Oyako_Saimin\\n',\n",
       "  'https://myanimelist.net/anime/49876/Mahou_Shoujo_Elena_DVD-BOX_Special\\n',\n",
       "  'https://myanimelist.net/anime/49762/Mama_x_Holic__Miwaku_no_Mama_to_Amaama_Kankei_-_The_Animation\\n'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the first 5 and last 5 anime url collected\n",
    "lines[:5], lines[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484e052-9ab9-47a9-b9cb-7817072863d7",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c504fff-a39a-410e-a325-860d2c4bcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "# for time view\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663c537-96db-4bbc-a6c1-4fe2a379ab83",
   "metadata": {},
   "source": [
    "### The Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6b6d2-00e8-43e8-8c07-0e774c564c77",
   "metadata": {},
   "source": [
    "We begin creating the pages needed to store the anime. \n",
    "\n",
    "The pages from 1 to 383 are stored in the Anime_pages folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9bc56-c8cc-45ac-82cb-1409286d0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for page in tqdm(range(1, 384)):\n",
    "    folder = \"page\"+str(page)\n",
    "    path = \"./Anime_pages/\"+folder\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2b2ba-cbf9-43b3-80b6-236cbe9e9d39",
   "metadata": {},
   "source": [
    "Now we create a double loop, over the pages and over the 50 anime, to collect the whole html and save it in the correct pages.\n",
    "\n",
    "For group division, here the loop goes from page 1 to 130. For have the complete loop we can iter over 0 to 383."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7997-b49f-43e2-8b83-5783ce7cd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in tqdm(range(0, 130)):  \n",
    "    # positioning in the right folder page\n",
    "    folder = \"/page\"+str(page+1)\n",
    "    for i in range(0,50):   \n",
    "        page_now = 50*page\n",
    "        # gather the url\n",
    "        url = lines[page_now+i]\n",
    "        response = requests.get(url)\n",
    "        # name of the file   \n",
    "        filename = r\"./Anime_pages\"+folder+\"/anime_\"+str(page_now+i+1)+\".txt\"\n",
    "        with open(filename,'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9023d-b58c-4a96-b938-cbfa10ceb891",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d1628-0f10-46e2-a1c6-be547cd0079f",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5732a725-0ec7-4b64-9c10-69e0371dd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "# usefull libraries to manage file in folder \n",
    "import os\n",
    "from natsort import natsorted\n",
    "# for time view\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd85e2-c6e8-4576-8e87-e2498a9667fd",
   "metadata": {},
   "source": [
    "Because there are vary information to collect about an anime, to have a clear notebook we split in functions our parsing in the \"functions.py\" file.\n",
    "\n",
    "We divide the crawl in three function (or three part):\n",
    "* scrabbing_anime1:  Title, Type, number of Episode, Realease / End date\n",
    "* scrabbing_anime2:  number of Members, Score, Users, Rank, Popularity\n",
    "* scrabbing_anime3:  Synopsis, anime Related, anime Characters and Voices, anime Staff\n",
    "\n",
    "Also we have a function called: \n",
    "* parse_time:  a function that parse the date collected from the html in a datetime object\n",
    "\n",
    "The parsing process is this: we create a list called \"anime_info\" in which we collect all the info about an anime. Than using this list we create a dataframe which columns are coming from the \"attrs\" list (a list containing the title of each attributes) and save it as a .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee40f334-20ac-4463-9e1c-b0116527625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the functions on functions.py file\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117aeb17-8606-4b8d-910e-a1df6d0fefee",
   "metadata": {},
   "source": [
    "Let's view the process with one anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3defaa7a-288b-403e-82da-2f7922a1fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the columns \n",
    "attrs = [\"animeTitle\", \"animeType\", \"animeNumEpisode\",\"releaseDate\",\"endDate\",\"animeNumMembers\",\"animeScore\",\"animeUsers\",\"animeRank\",\n",
    "         \"animePopularity\",\"animeDescription\",\"animeRelated\",\"animeCharacters\",\"animeVoices\",\"animeStaff\"]\n",
    "\n",
    "# positioning \n",
    "html_name = r\"./Anime_pages/page1/anime_1.html\"\n",
    "\n",
    "# take the html of the file \n",
    "with open(html_name, \"r\",  encoding='utf-8') as fp:\n",
    "    soup = BeautifulSoup(fp, \"html.parser\")\n",
    "\n",
    "# collecting the information\n",
    "anime_info = []\n",
    "anime_info + functions.scrabbing_anime1(soup, anime_info)\n",
    "anime_info + functions.scrabbing_anime2(soup, anime_info)\n",
    "anime_info + functions.scrabbing_anime3(soup, anime_info)\n",
    "\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame([anime_info], columns = attrs)\n",
    "# change attributes to str\n",
    "str_cols = [\"animeTitle\", \"animeType\", \"animeDescription\"]\n",
    "df[str_cols] = df[str_cols].astype(\"string\")\n",
    "# Creating the tsv file, take the anime number (the id)\n",
    "name = re.sub(\".html\",\"\",anime)\n",
    "df.to_csv(\"./tsv_anime/\"+name+\".tsv\", index = False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6f1305a-c0ae-4a1d-a477-4f04c025144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeType</th>\n",
       "      <th>animeNumEpisode</th>\n",
       "      <th>releaseDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>animeNumMembers</th>\n",
       "      <th>animeScore</th>\n",
       "      <th>animeUsers</th>\n",
       "      <th>animeRank</th>\n",
       "      <th>animePopularity</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeRelated</th>\n",
       "      <th>animeCharacters</th>\n",
       "      <th>animeVoices</th>\n",
       "      <th>animeStaff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>TV</td>\n",
       "      <td>64</td>\n",
       "      <td>2009-04-05</td>\n",
       "      <td>2010-07-04</td>\n",
       "      <td>2676066</td>\n",
       "      <td>9.16</td>\n",
       "      <td>1622384</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>[Fullmetal Alchemist: Brotherhood - 4-Koma The...</td>\n",
       "      <td>[Elric, Edward, Elric, Alphonse, Mustang, Roy,...</td>\n",
       "      <td>[Park, RomiJapanese, Kugimiya, RieJapanese, Mi...</td>\n",
       "      <td>[[[Cook, Justin, Producer], [Yonai, Noritomo, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         animeTitle animeType  animeNumEpisode releaseDate  \\\n",
       "0  Fullmetal Alchemist: Brotherhood        TV               64  2009-04-05   \n",
       "\n",
       "     endDate  animeNumMembers  animeScore  animeUsers  animeRank  \\\n",
       "0 2010-07-04          2676066        9.16     1622384          1   \n",
       "\n",
       "   animePopularity                                   animeDescription  \\\n",
       "0                3  After a horrific alchemy experiment goes wrong...   \n",
       "\n",
       "                                        animeRelated  \\\n",
       "0  [Fullmetal Alchemist: Brotherhood - 4-Koma The...   \n",
       "\n",
       "                                     animeCharacters  \\\n",
       "0  [Elric, Edward, Elric, Alphonse, Mustang, Roy,...   \n",
       "\n",
       "                                         animeVoices  \\\n",
       "0  [Park, RomiJapanese, Kugimiya, RieJapanese, Mi...   \n",
       "\n",
       "                                          animeStaff  \n",
       "0  [[[Cook, Justin, Producer], [Yonai, Noritomo, ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bb69295-701e-4e8e-afb4-b6e3a450de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   animeTitle        1 non-null      string        \n",
      " 1   animeType         1 non-null      string        \n",
      " 2   animeNumEpisode   1 non-null      int64         \n",
      " 3   releaseDate       1 non-null      datetime64[ns]\n",
      " 4   endDate           1 non-null      datetime64[ns]\n",
      " 5   animeNumMembers   1 non-null      int64         \n",
      " 6   animeScore        1 non-null      float64       \n",
      " 7   animeUsers        1 non-null      int64         \n",
      " 8   animeRank         1 non-null      int64         \n",
      " 9   animePopularity   1 non-null      int64         \n",
      " 10  animeDescription  1 non-null      string        \n",
      " 11  animeRelated      1 non-null      object        \n",
      " 12  animeCharacters   1 non-null      object        \n",
      " 13  animeVoices       1 non-null      object        \n",
      " 14  animeStaff        1 non-null      object        \n",
      "dtypes: datetime64[ns](2), float64(1), int64(5), object(4), string(3)\n",
      "memory usage: 248.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131c86d-1b47-4025-9021-82e2722e09e5",
   "metadata": {},
   "source": [
    "For all the anime we proceed in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62885c30-6e7d-4eb5-ad69-670ba16963ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = [\"animeTitle\", \"animeType\", \"animeNumEpisode\",\"releaseDate\",\"endDate\",\"animeNumMembers\",\"animeScore\",\"animeUsers\",\"animeRank\",\n",
    "         \"animePopularity\",\"animeDescription\",\"animeRelated\",\"animeCharacters\",\"animeVoices\",\"animeStaff\"]\n",
    "\n",
    "\n",
    "# iterate from page 1 to 384 \n",
    "# ( remember the page are from 1 to 383 and range goes from 1 to 384 )\n",
    "for page in tqdm(range(1,384)):\n",
    "    # positioning\n",
    "    folder = \"./Anime_pages/page\"+str(page)\n",
    "    # iterate over the \"ordered\" list of anime\n",
    "    for anime in natsorted(os.listdir(folder)):\n",
    "            # open the anime\n",
    "            with open(folder + \"/\" + anime, \"r\",  encoding='utf-8') as fp:\n",
    "                soup = BeautifulSoup(fp, \"html.parser\")\n",
    "            anime_info = []\n",
    "            anime_info + scrabbing_anime1(soup, anime_info)\n",
    "            anime_info + scrabbing_anime2(soup, anime_info)\n",
    "            anime_info + scrabbing_anime3(soup, anime_info)\n",
    "        \n",
    "            # Creating the DataFrame\n",
    "            df = pd.DataFrame([anime_info], columns = attrs)\n",
    "            # change attributes to str\n",
    "            str_cols = [\"animeTitle\", \"animeType\", \"animeDescription\"]\n",
    "            df[str_cols] = df[str_cols].astype(\"string\")\n",
    "            # Creating the tsv file, take the anime number (the id)\n",
    "            name = re.sub(\".html\",\"\",anime)\n",
    "            df.to_csv(\"./tsv_anime/\"+name+\".tsv\", index = False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c49f81-0fd8-440a-a588-a0dc20507dc9",
   "metadata": {},
   "source": [
    "# [EX2] Search Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe48fa2-04d5-4001-a03a-672a56b5fd19",
   "metadata": {},
   "source": [
    "## 2.0 Preprocessing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c38772-4dbd-407d-86da-66c0612dae2e",
   "metadata": {},
   "source": [
    "### Let's collect our corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c24fa-b23d-48b1-ad6c-43db28d8979d",
   "metadata": {},
   "source": [
    "First of all lets gather all the documents in one list.\n",
    "* documents: lists of document. Each document correspond to an anime description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ccd4e2-a8e2-4fab-a225-f501ae3cf37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b598de-7990-4afe-8b1e-7f63abd0ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19116/19116 [00:50<00:00, 380.37it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "# positioning\n",
    "folder = r\"./tsv_anime/\"\n",
    "# iter over the file\n",
    "for anime in tqdm(natsorted(os.listdir(folder))):\n",
    "    df = pd.read_csv(folder+anime, sep = \"\\t\")\n",
    "    # take only the description\n",
    "    documents.append(df[\"animeDescription\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb63ede-ab31-4e54-9955-492a0cde5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gintoki, Shinpachi, and Kagura return as the fun-loving but broke members of the Yorozuya team! Living in an alternate-reality Edo, where swords are prohibited and alien overlords have conquered Japan, they try to thrive on doing whatever work they can get their hands on. However, Shinpachi and Kagura still haven't been paid... Does Gin-chan really spend all that cash playing pachinko?\\n\\n\\nMeanwhile, when Gintoki drunkenly staggers home one night, an alien spaceship crashes nearby. A fatally injured crew member emerges from the ship and gives Gintoki a strange, clock-shaped device, warning him that it is incredibly powerful and must be safeguarded. Mistaking it for his alarm clock, Gintoki proceeds to smash the device the next morning and suddenly discovers that the world outside his apartment has come to a standstill. With Kagura and Shinpachi at his side, he sets off to get the device fixed; though, as usual, nothing is ever that simple for the Yorozuya team.\\n\\n\\nFilled with tongue-in-cheek humor and moments of heartfelt emotion, Gintama's fourth season finds Gintoki and his friends facing both their most hilarious misadventures and most dangerous crises yet.\\n\\n\\n[Written by MAL Rewrite]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a document \n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32864fa8-f9cf-4877-8e42-9c72e1f79853",
   "metadata": {},
   "source": [
    "### Clean the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db171cec-e83c-4045-a626-35c5af19995e",
   "metadata": {},
   "source": [
    "Now let's cleaning all the documents. This step is colled preprocessing. We follow this order:\n",
    "- 1) expand contraction type 1 + Normalization (capital lower words)\n",
    "- 2) splitting number from text (ex 25min) and removing contraction type 2 \n",
    "- 3) removing punctuation\n",
    "- 4) removing special characters\n",
    "- 3) Tokanize. We divide the string in words.\n",
    "- 4) removing stopwords\n",
    "- 5) removing some other words or non-text string\n",
    "- 6) lemmatizing and stemming \n",
    "\n",
    "For contraction type 1 and 2 see below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e62a5-b82f-46ac-b56f-ef9d9533b6fe",
   "metadata": {},
   "source": [
    "Let's inspect the document to see what we can delete and what no:\n",
    "- for example: \"Philosopher's Stone—a powerful\" << This is dash\n",
    "- but \"bio-mechanical engineering\" << This is hyphen\n",
    "- but we also have \"15-year\" << hyphen <br>\n",
    "\n",
    "We decide to remove the hypen and the dash\n",
    "\n",
    "We encounter also some special characters like: … • ♥ →☆‘ . We remove them.\n",
    "\n",
    "At the end of a lot of anime description there is a \"Written MAL Rewrite\". We remove that.\n",
    "\n",
    "When tokanizing there is also the saxon genitive \" 's \": we remove that.\n",
    "\n",
    "Also we encounter a lot of contraction form: using wikipedia https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n",
    "we store them in a dictionary and restore the long form.\n",
    "Using the same idea we have term like: min or sec, so we use a second dictionary to restore the original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a458cc30-9923-4b49-9fbc-23b3c521566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core libraries \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# functions.py\n",
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513616f7-5c6e-4bde-9b00-8710ab09c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall\",\n",
    "\"he'll've\": \"he shall have\",\n",
    "\"he's\": \"he has\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall\",\n",
    "\"i'll've\": \"I shall have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall\",\n",
    "\"it'll've\": \"it shall have\",\n",
    "\"it's\": \"it has\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall\",\n",
    "\"she'll've\": \"she shall have\",\n",
    "\"she's\": \"she has\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall\",\n",
    "\"they'll've\": \"they shall have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall\",\n",
    "\"what'll've\": \"what shall have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall\",\n",
    "\"who'll've\": \"who shall have\",\n",
    "\"who's\": \"who has\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall\",\n",
    "\"you'll've\": \"you shall have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a7976d3-d27f-4cee-a555-0d05b3421c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions2 = {\n",
    "\"min\": \"minute\",\n",
    "\"sec\": \"second\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52b75a8-7de8-46e6-9855-ba7c1c55cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(documents):\n",
    "    stop = stopwords.words(\"english\")\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    remove = [\"Written\", \"MAL\", \"Rewrite\"]+[\"'s\"]\n",
    "    \n",
    "    # removing contraction + Normalization\n",
    "    document_tmp = functions.replace_words(documents.lower(), contractions)\n",
    "    # splitting number and text \n",
    "    document_tmp = \" \".join(re.split(r\"([0-9]+)([a-z]+)\",document_tmp))\n",
    "    document_tmp = functions.replace_words(document_tmp, contractions2)\n",
    "    # removing punctuation\n",
    "    document_tmp = re.sub(r\"[{}\\—ー``\\\"'“''――]\".format(string.punctuation),\" \",document_tmp)\n",
    "    # removing special characters\n",
    "    document_tmp = re.sub(r\"[…•♥→☆‘★]\",\" \",document_tmp)\n",
    "    # Tokenizing \n",
    "    document_tmp = word_tokenize(document_tmp) \n",
    "    # removing stopwords\n",
    "    document_tmp = [ word for word in document_tmp if word not in stop]\n",
    "    # removing \"Written MAL Rewrite\" and other stuff\n",
    "    document_tmp = [ word for word in document_tmp if word not in remove]\n",
    "    # lemmatize\n",
    "    document_tmp = [ lmtzr.lemmatize(word) for word in document_tmp]\n",
    "    # stemming \n",
    "    document_tmp = [ snowball_stemmer.stem(word) for word in document_tmp]\n",
    "\n",
    "    return document_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225ad3d-98f0-4c27-8c23-ef270152b86c",
   "metadata": {},
   "source": [
    "- documents_clean: is a list of list of the documents cleaned. Each list contain the tokenize cleaning document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c530bb00-3b6d-4884-98dd-fc572a386b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the documents\n",
    "documents_clean = []\n",
    "for d in documents:\n",
    "    documents_clean.append(pre_processing(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07a57c-39ac-4dc6-a0ea-93ab9761e700",
   "metadata": {},
   "source": [
    "Let's view how a document is processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e76ba0-8066-434b-a4e6-a7a5ac1baed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horrif',\n",
       " 'alchemi',\n",
       " 'experi',\n",
       " 'go',\n",
       " 'wrong',\n",
       " 'elric',\n",
       " 'household',\n",
       " 'brother',\n",
       " 'edward',\n",
       " 'alphons']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_clean[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa627913-655d-40c8-ac0c-f25409e1386c",
   "metadata": {},
   "source": [
    "### Creating vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b567f44-d07a-44ab-a901-e8a4070661bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core libraries\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93517da8-3459-4164-a0dc-82dc1b689968",
   "metadata": {},
   "source": [
    "I will create a list of each unique word among the all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817d746d-419a-4959-9c2a-5c8396919692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of all unique words\n",
    "word_list = list(set(list(itertools.chain.from_iterable(documents_clean))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7251fe-abc5-4ac8-a9e3-f089eb58d171",
   "metadata": {},
   "source": [
    "Creating a dictionary that maps each word to an integer: we use the function zip to assign to each word an order integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cdfcdb5-a989-4cc9-ae82-b5db2e4f6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabolary = dict(zip(word_list, range(len(word_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c13dae-d80f-488e-8497-90be83723b25",
   "metadata": {},
   "source": [
    "Saving the dictionary in a .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce71aaea-a283-4696-b6f5-ae27e516f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open(\"vocabolary.json\", \"w\", encoding = \"utf-8\")\n",
    "json.dump(vocabolary, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e00037-fcd6-4a93-9e0c-dd8a92ef3dab",
   "metadata": {},
   "source": [
    "Import the saved vocabolary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfbab9ef-740a-43c2-a32d-b8cdfde670b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"vocabolary.json\" ) as f:\n",
    "    vocabolary = json.load( f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfdc7ba6-c11a-4492-8f4a-f9d32b027062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underworld --> 0\n",
      "niizato --> 1\n",
      "winri --> 2\n",
      "itali --> 3\n",
      "reinforc --> 4\n",
      "nth --> 5\n",
      "jeffri --> 6\n",
      "encor --> 7\n",
      "nyan --> 8\n",
      "wecom --> 9\n"
     ]
    }
   ],
   "source": [
    "# view the vocabolary\n",
    "count = 0\n",
    "for key, mapped_int in vocabolary.items():\n",
    "    count +=1\n",
    "    print(key,\"-->\",mapped_int)\n",
    "    if count == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af2280-f12e-4194-9a61-c3039ad9c01f",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf4a9e-2b5e-43e1-9c4e-8f80d20af9be",
   "metadata": {},
   "source": [
    "For this type of search engine we need only to have a search engine based on the query appear or not in each documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcf152-bd00-47c9-8d2a-02891d697773",
   "metadata": {},
   "source": [
    "* ### Prepare the mapped document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a73e6-1c67-4491-9a94-3a8f92f15476",
   "metadata": {},
   "source": [
    "To do this we will create an array of documents of each len(document_j) in which thare are converted the word into integer based on the vocabolary\n",
    "\n",
    "We will use numpy array for time optimitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bdcc7ef-4848-4100-97c5-e1428f4a9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_int(document, vocabolary):\n",
    "    int_doc = np.zeros(len(document), dtype = np.int64)\n",
    "    # iterating over the document that has len(d)<<len(vocabolary)\n",
    "    # change the value of the document, otherwise remain zero\n",
    "    for i, word in enumerate(document):\n",
    "        # vocabolary[word] is the mapping function that return an integer i.e the index\n",
    "        int_doc[i] = vocabolary[word]\n",
    "        \n",
    "    return np.sort(int_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63c49f-534e-484d-a5cc-9227e4ee1551",
   "metadata": {},
   "source": [
    "* documents_mapped: is a list of list that have the words mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07504e7d-eba2-47ff-8532-b7169d3d3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_mapped = []\n",
    "for d in documents_clean:\n",
    "    documents_mapped.append(word_to_int(d,vocabolary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6aedaf5-5784-4750-a51e-97b224739d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     2,   223,   259,   259,   275,   350,   350,   350,\n",
       "         350,   807,   879,  1026,  1319,  1504,  1780,  1785,  1891,\n",
       "        3189,  4042,  4071,  4271,  4430,  4642,  5163,  5296,  5874,\n",
       "        6255,  6667,  7065,  7065,  7797,  7974,  8053,  8272,  8589,\n",
       "        8628,  8813,  9466,  9658,  9658,  9658,  9834, 10621, 11227,\n",
       "       11227, 11553, 12827, 12881, 13535, 13926, 13950, 14612, 14694,\n",
       "       15308, 15330, 15972, 16074, 16713, 16791, 16895, 18092, 18630,\n",
       "       18952, 19058, 19241, 19353, 19378, 19637, 19689, 19810, 20020,\n",
       "       20063, 20465, 21067, 21827, 21923, 22139, 22309, 23280, 23307,\n",
       "       23844, 23878, 25024, 25158, 25619, 25619, 26391, 28726, 28764,\n",
       "       28928, 29272, 29323, 29604, 29834, 29933, 30841, 31158, 31158,\n",
       "       31184, 31217, 31567, 31599, 32901, 32909, 33917, 34129, 34129,\n",
       "       34129, 34234, 34520, 34569, 34787, 35029], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a docoument mapped \n",
    "documents_mapped[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fef522-d5f7-4127-964c-cd95014d209e",
   "metadata": {},
   "source": [
    "* ### Inverted Index v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91eb8356-08f9-4b2b-9a0a-026601dc5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b4d7eb1-6ddf-41ed-a693-5066ef7f26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Inverted_index\n",
    "Inverted_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79645f42-7309-4f1f-9dba-f9894e19fcbd",
   "metadata": {},
   "source": [
    "To compute the Inverted Index we iterating over each document. Every time we encounter a word (that is now a integer) we insert in the dictionary the id of the documents, which is the row index in documents_mapped or in our dataset of tsv / url.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eed87ad4-db34-45a2-8a68-5e9dc769976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(documents_mapped):\n",
    "    for word in set(d):\n",
    "        Inverted_index[str(word)].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f63a71d0-fffa-4021-990c-6a9af19575c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  2 --> documents:  [0, 525, 4155]\n",
      "word:  1026 --> documents:  [0, 172, 356, 363, 414, 449, 530, 685, 693, 709, 727, 985, 1060, 1066, 1777, 1811, 1852, 2078, 2393, 2550, 3361, 3802, 3978, 3987, 4009, 4227, 4247, 4486, 4595, 4657, 5291, 5434, 5775, 6291, 6461, 6689, 7701, 7880, 8556, 8583, 8810, 8925, 9306, 9673, 9714, 10054, 10232, 10485, 10709, 11005, 11152, 11220, 11291, 12254, 12989, 14429, 14606, 16502, 17348]\n"
     ]
    }
   ],
   "source": [
    "# let's view the Inverted_index\n",
    "count = 0\n",
    "for key, lis in Inverted_index.items():\n",
    "    count +=1\n",
    "    print(\"word: \",key,\"-->\",\"documents: \", lis)\n",
    "    if count == 2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149ab26-bdb5-43a8-9ebe-baf6df8d3047",
   "metadata": {},
   "source": [
    "Saving the Inverted Index in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82af7614-dafb-4070-ba00-a21b7063c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open(\"Inverted_index_v1.json\", \"w\", encoding = \"utf-8\")\n",
    "json.dump(Inverted_index, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4647494-4d12-41b9-9cdf-988294388ff9",
   "metadata": {},
   "source": [
    "Import the saved inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f6587f-77f5-4eab-8d76-fd6da702e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"Inverted_index_v1.json\" ) as f:\n",
    "    Inverted_index = json.load( f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b6073-4de3-4a06-bd0c-01ac37755287",
   "metadata": {},
   "source": [
    "* ### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3df98-66da-4612-bc57-413000bef3f7",
   "metadata": {},
   "source": [
    "How it work:\n",
    "\n",
    "* Given in input a query we first of all preprocessing like a document;\n",
    "* then using the Inverted index we exctract the list corrisponding to each element (word) of the query, store them in a list of set (for intersection pourpose) called \"index\". \n",
    "* Than we intersect all the set for obtain the documents that match ALL the query elements.\n",
    "* In the end we create the dataframe with the desire output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92d64cd9-f197-4c3c-9479-204a5028a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_v1(query_text):\n",
    "    # pre-processing the query\n",
    "    query_clean = pre_processing(query_text)\n",
    "    query_int = word_to_int(query_clean, vocabolary)\n",
    "    \n",
    "    # for each element of the query we can obtain the list of doc \n",
    "    # in which the query element appears\n",
    "    index = []\n",
    "    \n",
    "    for query in query_int:\n",
    "        # creating a set of index (set is for intersection pourpose)\n",
    "        index.append(set(Inverted_index[str(query)]))\n",
    "    \n",
    "    # intersect and obtain the documents that contain ALL the query\n",
    "    index = list(index[0].intersection(*index))\n",
    "    \n",
    "    # taken the url of the anime list\n",
    "    with open(\"./anime_url.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    # we are searching for the anime and the url corrisponding to the index\n",
    "    # we found\n",
    "    anime_path = []\n",
    "    url = []\n",
    "\n",
    "    for idx in index:\n",
    "        # we need the +1 because we start indexing from 1\n",
    "        name = \"/anime_\"+str(idx+1)+\".tsv\"\n",
    "        anime_path.append(name)\n",
    "        url.append(lines[idx])\n",
    "\n",
    "    # creating the dataframe for view the result\n",
    "    animes_df = []\n",
    "    # folder of the anime_tsv\n",
    "    folder = r\"./tsv_anime/\"\n",
    "    # column I want\n",
    "    cols = [\"animeTitle\",\"animeDescription\"]\n",
    "    for i,anime_tsv in enumerate(anime_path):\n",
    "        df = pd.read_csv(folder+anime_tsv, sep = \"\\t\", usecols = cols)\n",
    "        # creating new column with the url\n",
    "        df[\"animeURL\"] = url[i]\n",
    "        animes_df.append(df)\n",
    "    \n",
    "    frame = pd.concat(animes_df, ignore_index = True)\n",
    "    display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c87806-b9d0-4b94-ae76-79f2c1aad4ee",
   "metadata": {},
   "source": [
    "#### Input and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "239fd85f-aadd-47db-9475-42cbf6226172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert the query:  alchemist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/5114/Fullmetal_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senki Zesshou Symphogear GX</td>\n",
       "      <td>Following the events of Senki Zesshou Symphoge...</td>\n",
       "      <td>https://myanimelist.net/anime/21573/Senki_Zess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gosick</td>\n",
       "      <td>Kazuya Kujou is a foreign student at Saint Mar...</td>\n",
       "      <td>https://myanimelist.net/anime/8425/Gosick\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Garo: Vanishing Line</td>\n",
       "      <td>Corruption looms over the prosperous Russell C...</td>\n",
       "      <td>https://myanimelist.net/anime/36144/Garo__Vani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fullmetal Alchemist</td>\n",
       "      <td>Edward Elric, a young, brilliant alchemist, ha...</td>\n",
       "      <td>https://myanimelist.net/anime/121/Fullmetal_Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood Specials</td>\n",
       "      <td>Amazing secrets and startling facts are expose...</td>\n",
       "      <td>https://myanimelist.net/anime/6421/Fullmetal_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arcana Famiglia: Capriccio - stile Arcana Fami...</td>\n",
       "      <td>After toiling away in his lab, the alchemist J...</td>\n",
       "      <td>https://myanimelist.net/anime/15411/Arcana_Fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ulysses: Jehanne Darc to Renkin no Kishi</td>\n",
       "      <td>The story is set in the 15th century, during t...</td>\n",
       "      <td>https://myanimelist.net/anime/36510/Ulysses__J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shinmai Renkinjutsushi no Tenpo Keiei</td>\n",
       "      <td>Shoot for the stars! I'm going to be the count...</td>\n",
       "      <td>https://myanimelist.net/anime/49849/Shinmai_Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bungou to Alchemist: Shinpan no Haguruma</td>\n",
       "      <td>Famous writers throughout history find themsel...</td>\n",
       "      <td>https://myanimelist.net/anime/40934/Bungou_to_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fullmetal Alchemist: The Sacred Star of Milos</td>\n",
       "      <td>Chasing a runaway alchemist with strange power...</td>\n",
       "      <td>https://myanimelist.net/anime/9135/Fullmetal_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baccano!</td>\n",
       "      <td>During the early 1930s in Chicago, the transco...</td>\n",
       "      <td>https://myanimelist.net/anime/2251/Baccano\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senki Zesshou Symphogear AXZ</td>\n",
       "      <td>Hibiki Tachibana has defeated many powerful en...</td>\n",
       "      <td>https://myanimelist.net/anime/32836/Senki_Zess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Garo: Honoo no Kokuin</td>\n",
       "      <td>In the name of the king, the Valiante Kingdom ...</td>\n",
       "      <td>https://myanimelist.net/anime/23311/Garo__Hono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Birthday Wonderland</td>\n",
       "      <td>The day before her birthday, Akane is asked to...</td>\n",
       "      <td>https://myanimelist.net/anime/38985/Birthday_W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fullmetal Alchemist: Premium Collection</td>\n",
       "      <td>1. State Alchemists vs Seven Homunculi\\n\\nA 10...</td>\n",
       "      <td>https://myanimelist.net/anime/908/Fullmetal_Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Garo: Guren no Tsuki</td>\n",
       "      <td>Monsters known as \"Horrors\" have invaded the w...</td>\n",
       "      <td>https://myanimelist.net/anime/28537/Garo__Gure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fullmetal Alchemist: The Sacred Star of Milos ...</td>\n",
       "      <td>To mark the July 2 opening of the Fullmetal Al...</td>\n",
       "      <td>https://myanimelist.net/anime/10842/Fullmetal_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           animeTitle  \\\n",
       "0                    Fullmetal Alchemist: Brotherhood   \n",
       "1                         Senki Zesshou Symphogear GX   \n",
       "2                                              Gosick   \n",
       "3                                Garo: Vanishing Line   \n",
       "4                                 Fullmetal Alchemist   \n",
       "5           Fullmetal Alchemist: Brotherhood Specials   \n",
       "6   Arcana Famiglia: Capriccio - stile Arcana Fami...   \n",
       "7            Ulysses: Jehanne Darc to Renkin no Kishi   \n",
       "8               Shinmai Renkinjutsushi no Tenpo Keiei   \n",
       "9            Bungou to Alchemist: Shinpan no Haguruma   \n",
       "10      Fullmetal Alchemist: The Sacred Star of Milos   \n",
       "11                                           Baccano!   \n",
       "12                       Senki Zesshou Symphogear AXZ   \n",
       "13                              Garo: Honoo no Kokuin   \n",
       "14                                Birthday Wonderland   \n",
       "15            Fullmetal Alchemist: Premium Collection   \n",
       "16                               Garo: Guren no Tsuki   \n",
       "17  Fullmetal Alchemist: The Sacred Star of Milos ...   \n",
       "\n",
       "                                     animeDescription  \\\n",
       "0   After a horrific alchemy experiment goes wrong...   \n",
       "1   Following the events of Senki Zesshou Symphoge...   \n",
       "2   Kazuya Kujou is a foreign student at Saint Mar...   \n",
       "3   Corruption looms over the prosperous Russell C...   \n",
       "4   Edward Elric, a young, brilliant alchemist, ha...   \n",
       "5   Amazing secrets and startling facts are expose...   \n",
       "6   After toiling away in his lab, the alchemist J...   \n",
       "7   The story is set in the 15th century, during t...   \n",
       "8   Shoot for the stars! I'm going to be the count...   \n",
       "9   Famous writers throughout history find themsel...   \n",
       "10  Chasing a runaway alchemist with strange power...   \n",
       "11  During the early 1930s in Chicago, the transco...   \n",
       "12  Hibiki Tachibana has defeated many powerful en...   \n",
       "13  In the name of the king, the Valiante Kingdom ...   \n",
       "14  The day before her birthday, Akane is asked to...   \n",
       "15  1. State Alchemists vs Seven Homunculi\\n\\nA 10...   \n",
       "16  Monsters known as \"Horrors\" have invaded the w...   \n",
       "17  To mark the July 2 opening of the Fullmetal Al...   \n",
       "\n",
       "                                             animeURL  \n",
       "0   https://myanimelist.net/anime/5114/Fullmetal_A...  \n",
       "1   https://myanimelist.net/anime/21573/Senki_Zess...  \n",
       "2         https://myanimelist.net/anime/8425/Gosick\\n  \n",
       "3   https://myanimelist.net/anime/36144/Garo__Vani...  \n",
       "4   https://myanimelist.net/anime/121/Fullmetal_Al...  \n",
       "5   https://myanimelist.net/anime/6421/Fullmetal_A...  \n",
       "6   https://myanimelist.net/anime/15411/Arcana_Fam...  \n",
       "7   https://myanimelist.net/anime/36510/Ulysses__J...  \n",
       "8   https://myanimelist.net/anime/49849/Shinmai_Re...  \n",
       "9   https://myanimelist.net/anime/40934/Bungou_to_...  \n",
       "10  https://myanimelist.net/anime/9135/Fullmetal_A...  \n",
       "11       https://myanimelist.net/anime/2251/Baccano\\n  \n",
       "12  https://myanimelist.net/anime/32836/Senki_Zess...  \n",
       "13  https://myanimelist.net/anime/23311/Garo__Hono...  \n",
       "14  https://myanimelist.net/anime/38985/Birthday_W...  \n",
       "15  https://myanimelist.net/anime/908/Fullmetal_Al...  \n",
       "16  https://myanimelist.net/anime/28537/Garo__Gure...  \n",
       "17  https://myanimelist.net/anime/10842/Fullmetal_...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input query \n",
    "query_text = input(\"Insert the query: \")\n",
    "\n",
    "search_engine_v1(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1666fe-7af7-4342-b656-53e7139f5219",
   "metadata": {},
   "source": [
    "# 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439542d-3a9e-41a8-988c-08e51515b310",
   "metadata": {},
   "source": [
    "Now we want for our Inverted_index two element:\n",
    "\n",
    "* $\\text{tf}_{i,j}$: occurancy of term $j$ in document $i$\n",
    "* $\\text{idf}_{j}$: Inverse Document Frequency of term $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21b408-1040-4301-98e3-7c39eea2d7b0",
   "metadata": {},
   "source": [
    "Define:\n",
    "\n",
    "* n_words = total number of words in vocabolary\n",
    "* n = number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa2fee55-53c3-4652-87cc-98766fb41c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(documents)\n",
    "n_words = len(vocabolary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05690d2a-4b89-4b23-ac20-a0b8ca88a8a0",
   "metadata": {},
   "source": [
    "Creating the $\\text{tf}_{i,j}$ matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c63be6-41e0-41b0-b858-4bb049d15e7a",
   "metadata": {},
   "source": [
    "* ### Prepare the mapped document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab231d-791e-4956-8a02-bd45e2f761e8",
   "metadata": {},
   "source": [
    "To do this we will create an array of documents of each len(document_j) in which thare are converted the word into integer based on the vocabolary\n",
    "\n",
    "We will use numpy array for time optimitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4932e208-917d-4346-b32f-cb6918c5e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that map document text to the relative tf\n",
    "\n",
    "def word_to_int2(document, vocabolary):\n",
    "    int_doc = np.zeros(len(vocabolary), dtype = np.int64)\n",
    "    # iterating over the document that has len(d)<<len(vocabolary)\n",
    "    # change the value of the document, otherwise remain zero\n",
    "    for word in document:\n",
    "        # vocabolary[word] is the mapping function that return an integer i.e the index\n",
    "        int_doc[vocabolary[word]] += 1\n",
    "        \n",
    "    return int_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9234a-8157-4b73-ba6f-4468237c8f36",
   "metadata": {},
   "source": [
    "* $tf$: is a list of list that have the words mapped with is count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2d33628-d1f9-45be-9440-ef8ce8d4a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = []\n",
    "for d in documents_clean:\n",
    "    tf.append(word_to_int2(d,vocabolary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42ec7e3d-cf53-49fb-a503-844a1ca69bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a doc\n",
    "tf[0][tf[0]>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5ddbe-0926-4cb3-a67c-ffd6025084e5",
   "metadata": {},
   "source": [
    "Creating the $\\text{idf}_{j}$ array:\n",
    "* I need the $n_j$: number of documents containing term j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "607f1ac3-d808-45e6-869c-b9bb42cebdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nj = np.zeros(n_words, dtype = np.float64)\n",
    "\n",
    "# we create a numpy array of length n_words\n",
    "# for each docoument we take the unique set of word\n",
    "# and count\n",
    "for d in documents_mapped:\n",
    "    for word in set(d):\n",
    "        nj[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eeadabd9-012f-42a1-9f64-bec065de5cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.,  1.,  3., ...,  1., 77.,  5.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view\n",
    "nj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83a840be-d3e4-4c75-95c9-ee4ee11a216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating idf_j\n",
    "\n",
    "idf = np.zeros(n_words, dtype = np.float64)\n",
    "idf = np.log(n/nj) / np.log(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8fced-f7bf-427f-bbe4-22378830f385",
   "metadata": {},
   "source": [
    "$\\text{tfIdf}_{ij} = \\text{tf}_{ij} * \\text{idf}_{j} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca7995f5-e70c-471f-b9b2-86fb4ea5e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdf = np.multiply(tf,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e54440fc-9659-4446-871a-622eff7ac9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19116, 36056)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcffd8-a8e2-44ee-9747-e16296a6fabc",
   "metadata": {},
   "source": [
    "* ### Inverted index v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b41e71d9-c111-4cea-a5a6-eb67a9d4c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1993279-3ab9-4f5a-8c47-a0dc13889869",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inverted_indexv2 = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b0f26-90f6-412f-a99f-bfdfb68cb4ef",
   "metadata": {},
   "source": [
    "To compute the Inverted Index we iterating over each document. Every time we encounter a word (that is now a integer) we insert in the dictionary the __tupla__ of _id_ of the documents and the _tfIdf_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dbbc6bf-ed91-4ea4-b5af-a520a9346fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(documents_mapped):\n",
    "    for word in set(d):\n",
    "        # append (idd_i, tfidf_ij)\n",
    "        Inverted_indexv2[str(word)].append((i,tfIdf[i,int(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c39f6763-01a7-4da0-bd61-135beb9a3fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --> [[0, 1.7771188926021926], [525, 0.8885594463010963], [4155, 0.8885594463010963]]\n"
     ]
    }
   ],
   "source": [
    "# view\n",
    "count = 0\n",
    "for key, lis in Inverted_indexv2.items():\n",
    "    count +=1\n",
    "    print(key,\"-->\",lis)\n",
    "    if count == 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226b211-53fc-4f53-83ab-b0fa96d8ce8b",
   "metadata": {},
   "source": [
    "Saving the Inverted Index in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bca45c3-c2cf-485d-a6b6-8cec11c42d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open(\"Inverted_index_v2.json\", \"w\", encoding = \"utf-8\")\n",
    "json.dump(Inverted_indexv2, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e575920-efa2-4419-9ebc-bfbbb347064c",
   "metadata": {},
   "source": [
    "Import the saved inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cf55b5e-0619-4959-952b-d17a50a95934",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"Inverted_index_v2.json\" ) as f:\n",
    "    Inverted_indexv2 = json.load( f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4900ac9f-0b70-4c29-8fc6-92c69ccb8212",
   "metadata": {},
   "source": [
    "* ### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834ad45-9fc0-46f1-9794-a4f7fe5dc338",
   "metadata": {},
   "source": [
    "To better have a use of the Inverted Index + Ranking we suppose that a __query is input as a text of unique word__.\n",
    "\n",
    "We define than the cosine similarity as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\cos(q,d^i) =  \\frac{\\sum_{j: \\hspace{0.1cm}  q_j=1} \\text{tfIdf}_{ij}}{||d^i||*||q||}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Under this assumption we observe that: $||q|| = \\sqrt(len(q)$\n",
    "\n",
    "Recall: $d^i = [\\text{tfIdf}_{i1}, \\text{tfIdf}_{i2}, \\ldots]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4a1776e-65fb-4c75-a43c-662ebc8d56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to compute the manage the heap\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b132a34-94f5-4db8-a466-77e96e68b218",
   "metadata": {},
   "source": [
    "How is it work:\n",
    "\n",
    "* we take a query text in input and preprocessing like a normal document. Than we obtain the \"query_int\" array of 0 and 1: q[j] = 1 if word_j in q, 0 otherwise;\n",
    "* then using the Inverted index we exctract the list of tuple corrisponding to each element (word) of the query. We store also the len of each match list and the maximum list (we need it to manage the pointer of different length list);\n",
    "* We now have a function \"intersection_all\" that return two things: a boolean variable, enough that is True if I have at least k document that match the all query and match list, that return the list of the index that match all the query if true or the original match list if False;\n",
    "* Than the algorithm goes in two part:<br>\n",
    "$\\hspace{1cm}$  1. if True (I have at least k document) we compute the score of element in the list and return the top k<br>\n",
    "$\\hspace{1cm}$  2. if False, we compute the score af all the document and return the score ordered of the matches\n",
    "* In the end: we create the dataframe of the documents ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5656c09-fc6f-4c40-bc75-558480b0f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine2(query, k, tfIdf):\n",
    "    # preprocessing the query\n",
    "    query_clean = pre_processing(query)\n",
    "    query_int = word_to_int2(query_clean, vocabolary)\n",
    "\n",
    "    # initialize a match_list which store the list of match\n",
    "    match_list = []\n",
    "    # list of the length of the match \n",
    "    lenMatch = []\n",
    "    # tupla = (#list, len(list))\n",
    "    max_lenMatch = (0,-1)\n",
    "    for i,query in enumerate(np.where(query_int>0)[0]):\n",
    "        lis = Inverted_indexv2[str(query)]\n",
    "        match_list.append(lis)\n",
    "        tmplis, tmplen = i, len(lis)\n",
    "        if tmplen>max_lenMatch[1]:\n",
    "            max_lenMatch = (tmplis,tmplen)\n",
    "        lenMatch.append(len(lis))\n",
    "    \n",
    "    enough, match_list = functions.intersection_all(k, match_list)\n",
    "    m = len(query_clean)\n",
    "    if enough:\n",
    "        scores = functions.scoresK(match_list, tfIdf, m, query_int)\n",
    "        topscore, topk = functions.find_topK(k, scores)\n",
    "    else:\n",
    "        scores = functions.scoresALL(match_list, tfIdf, m, lenMatch)\n",
    "        k = len(scores)\n",
    "        topscore, topk = functions.find_topK(k, scores)\n",
    "            \n",
    "    # search the url\n",
    "    with open(\"./anime_url.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    # we are searching for the anime and the url \n",
    "    anime_path = []\n",
    "    url = []\n",
    "\n",
    "    for idx in topk:\n",
    "        # we need the +1 because we start indexing from 1\n",
    "        name = \"/anime_\"+str(idx+1)+\".tsv\"\n",
    "        anime_path.append(name)\n",
    "        url.append(lines[idx])\n",
    "\n",
    "    # creating the dataframe for view the result\n",
    "    animes_df = []\n",
    "    # folder of the anime_tsv\n",
    "    folder = r\"./tsv_anime/\"\n",
    "    # column I want\n",
    "    cols = [\"animeTitle\",\"animeDescription\"]\n",
    "    for i,anime_tsv in enumerate(anime_path):\n",
    "        df = pd.read_csv(folder+anime_tsv, sep = \"\\t\", usecols = cols)\n",
    "        # creating new column\n",
    "        df[\"animeURL\"] = url[i]\n",
    "        df[\"animeScores\"] = topscore[i]\n",
    "        animes_df.append(df)\n",
    "    \n",
    "    frame = pd.concat(animes_df, ignore_index = True)\n",
    "    display(frame)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fc639-05ce-462e-85a5-8030cc4ed7f7",
   "metadata": {},
   "source": [
    "### Input and searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "41252fe9-293e-4f1b-bbf0-ef286c14f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert the query:  alchemist alchemy\n",
      "Insert k:  5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeURL</th>\n",
       "      <th>animeScores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist</td>\n",
       "      <td>Edward Elric, a young, brilliant alchemist, ha...</td>\n",
       "      <td>https://myanimelist.net/anime/121/Fullmetal_Al...</td>\n",
       "      <td>0.399861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood Specials</td>\n",
       "      <td>Amazing secrets and startling facts are expose...</td>\n",
       "      <td>https://myanimelist.net/anime/6421/Fullmetal_A...</td>\n",
       "      <td>0.268511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ulysses: Jehanne Darc to Renkin no Kishi</td>\n",
       "      <td>The story is set in the 15th century, during t...</td>\n",
       "      <td>https://myanimelist.net/anime/36510/Ulysses__J...</td>\n",
       "      <td>0.258751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/5114/Fullmetal_A...</td>\n",
       "      <td>0.208161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baccano!</td>\n",
       "      <td>During the early 1930s in Chicago, the transco...</td>\n",
       "      <td>https://myanimelist.net/anime/2251/Baccano\\n</td>\n",
       "      <td>0.189374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  animeTitle  \\\n",
       "0                        Fullmetal Alchemist   \n",
       "1  Fullmetal Alchemist: Brotherhood Specials   \n",
       "2   Ulysses: Jehanne Darc to Renkin no Kishi   \n",
       "3           Fullmetal Alchemist: Brotherhood   \n",
       "4                                   Baccano!   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  Edward Elric, a young, brilliant alchemist, ha...   \n",
       "1  Amazing secrets and startling facts are expose...   \n",
       "2  The story is set in the 15th century, during t...   \n",
       "3  After a horrific alchemy experiment goes wrong...   \n",
       "4  During the early 1930s in Chicago, the transco...   \n",
       "\n",
       "                                            animeURL  animeScores  \n",
       "0  https://myanimelist.net/anime/121/Fullmetal_Al...     0.399861  \n",
       "1  https://myanimelist.net/anime/6421/Fullmetal_A...     0.268511  \n",
       "2  https://myanimelist.net/anime/36510/Ulysses__J...     0.258751  \n",
       "3  https://myanimelist.net/anime/5114/Fullmetal_A...     0.208161  \n",
       "4       https://myanimelist.net/anime/2251/Baccano\\n     0.189374  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = input(\"Insert the query: \")\n",
    "k = int(input(\"Insert k: \"))\n",
    "\n",
    "search_engine2(query, k, tfIdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
