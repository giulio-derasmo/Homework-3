{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc743901-5a7c-46df-a19c-827488ca49d5",
   "metadata": {},
   "source": [
    "# Search engine Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2601dbc6-b651-4662-adee-da67413a5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a9602-f54a-47e5-af8c-5d74e1ad1420",
   "metadata": {},
   "source": [
    "# Creating the Search Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2769ac-d7ae-42d1-95df-616d80e9d2a3",
   "metadata": {},
   "source": [
    "First of all lets gather all the documents in one list.\n",
    "* documents: lists of document. Each document correspond to an anime description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28da2fc2-ed9d-4ae6-ad2e-b40fe158050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6500/6500 [01:15<00:00, 86.02it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "folder = r\"./tsv_anime/\"\n",
    "for anime in tqdm(natsorted(os.listdir(folder))):\n",
    "    df = pd.read_csv(folder+anime, sep = \"\\t\")\n",
    "    documents.append(df[\"animeDescription\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593b3b4a-139f-48a6-adf2-d9e8f42d4216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gintoki, Shinpachi, and Kagura return as the fun-loving but broke members of the Yorozuya team! Living in an alternate-reality Edo, where swords are prohibited and alien overlords have conquered Japan, they try to thrive on doing whatever work they can get their hands on. However, Shinpachi and Kagura still haven't been paid... Does Gin-chan really spend all that cash playing pachinko?\\n\\n\\nMeanwhile, when Gintoki drunkenly staggers home one night, an alien spaceship crashes nearby. A fatally injured crew member emerges from the ship and gives Gintoki a strange, clock-shaped device, warning him that it is incredibly powerful and must be safeguarded. Mistaking it for his alarm clock, Gintoki proceeds to smash the device the next morning and suddenly discovers that the world outside his apartment has come to a standstill. With Kagura and Shinpachi at his side, he sets off to get the device fixed; though, as usual, nothing is ever that simple for the Yorozuya team.\\n\\n\\nFilled with tongue-in-cheek humor and moments of heartfelt emotion, Gintama's fourth season finds Gintoki and his friends facing both their most hilarious misadventures and most dangerous crises yet.\\n\\n\\n[Written by MAL Rewrite]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a document \n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068f185-cbf6-4c89-9c24-780f09f075a5",
   "metadata": {},
   "source": [
    "## Clean the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbdc6c4-2a3e-41b2-95d8-8e3866dc691b",
   "metadata": {},
   "source": [
    "Now let's cleaning all the documents. This step is colled preprocessing. We follow this order:\n",
    "- 1) expand contraction form + Normalization (capital lower words)\n",
    "- 2) remove number from text. We want only text string \n",
    "- 3) Tokanize. We divide the string in words.\n",
    "- 4) removing stopwords\n",
    "- 5) removing punctuation\n",
    "- 6) removing some other words or non-text string\n",
    "- 7) stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2677cc-59f9-46a8-b410-e1e50a816629",
   "metadata": {},
   "source": [
    "Let's inspect the document to see what we can delete and what no:\n",
    "- for example: \"Philosopher's Stone—a powerful\" << This is dash\n",
    "- but \"bio-mechanical engineering\" << This is hyphen\n",
    "\n",
    "We decide to keep the hypen and remove the dash\n",
    "\n",
    "Also we encounter a lot of contraction form: using wikipedia https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n",
    "we store them in a dictionary and restore the long form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2273c321-8e63-466d-b16d-26c237e264df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d61e8eb-04df-4713-87e4-72cc125906aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall\",\n",
    "\"he'll've\": \"he shall have\",\n",
    "\"he's\": \"he has\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall\",\n",
    "\"i'll've\": \"I shall have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall\",\n",
    "\"it'll've\": \"it shall have\",\n",
    "\"it's\": \"it has\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall\",\n",
    "\"she'll've\": \"she shall have\",\n",
    "\"she's\": \"she has\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall\",\n",
    "\"they'll've\": \"they shall have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall\",\n",
    "\"what'll've\": \"what shall have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall\",\n",
    "\"who'll've\": \"who shall have\",\n",
    "\"who's\": \"who has\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall\",\n",
    "\"you'll've\": \"you shall have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def replace_words(d, contractions):\n",
    "    for key, value in contractions.items():\n",
    "        d = d.replace(key, value)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "65281d90-6621-4b32-bbf6-b48abc2c913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(documents):\n",
    "    stop = stopwords.words(\"english\")\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    remove = [\"Written\", \"MAL\", \"Rewrite\"]+[\"'s\"]+[\"``\",'\"',\"'\",\"“\",\"''\"]\n",
    "\n",
    "    \n",
    "    # removing contraction + Normalization\n",
    "    document_tmp = replace_words(documents.lower(), contractions)\n",
    "    # remove number ( ordinal number too) \n",
    "    document_tmp = re.sub(r'[0-9]+(?:st|nd|rd|th)', '', document_tmp)\n",
    "    # remove dash \"—\"\n",
    "    document_tmp = document_tmp.replace('—',' ')\n",
    "    # Tokenizing \n",
    "    document_tmp =  word_tokenize(document_tmp) \n",
    "    # removing stopwords\n",
    "    document_tmp = [ word for word in document_tmp if word not in stop]\n",
    "    # removing punctuation\n",
    "    document_tmp = [ word for word in document_tmp if word not in string.punctuation]\n",
    "    # removing \"Written MAL Rewrite\" and other stuff\n",
    "    document_tmp = [ word for word in document_tmp if word not in remove]\n",
    "    # lemmatize\n",
    "    document_tmp = [ lmtzr.lemmatize(word) for word in document_tmp]\n",
    "    # stemming \n",
    "    document_tmp = [ snowball_stemmer.stem(word) for word in document_tmp]\n",
    "\n",
    "    \n",
    "    return document_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7df27-6f94-48c8-b4f7-3ef0c4ae4724",
   "metadata": {},
   "source": [
    "- documents_clean: is a list of list of the documents cleaned. Each list contain the tokenize cleaning document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a028b058-9f22-4ea3-87e6-ec58fddcb9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the documents\n",
    "documents_clean = []\n",
    "for d in documents:\n",
    "    documents_clean.append(pre_processing(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d015970-671c-4d90-a4b6-6232b6093e6a",
   "metadata": {},
   "source": [
    "Let's view how a document is processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1fb606de-f5e8-4b92-9356-46aef73d3b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horrif',\n",
       " 'alchemi',\n",
       " 'experi',\n",
       " 'go',\n",
       " 'wrong',\n",
       " 'elric',\n",
       " 'household',\n",
       " 'brother',\n",
       " 'edward',\n",
       " 'alphons']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_clean[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc32eac-77c4-4e8e-a635-daa8bbb9ee56",
   "metadata": {},
   "source": [
    "# Creating vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cad07d1f-2919-4b7b-b632-3e0823c49dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6aff5-25f4-466f-90f3-7488bf417b19",
   "metadata": {},
   "source": [
    "I will create a list of each unique word among the all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d318ef46-339d-4594-9197-05dc90ce1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of all words\n",
    "word_list = list(set(list(itertools.chain.from_iterable(documents_clean))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83f12b-ed4b-453e-8f49-548245d56acc",
   "metadata": {},
   "source": [
    "Creating a dictionary that maps each word to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d94d5fd1-7a84-4278-bd05-7f48e30d0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabolary = dict(zip(word_list, range(len(word_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a2da4690-0e16-4d33-acfb-e4953f4a960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forti --> 0\n",
      "fabiola --> 1\n",
      "gamer --> 2\n",
      "alternate-histori --> 3\n",
      "savor --> 4\n",
      "blue-hair --> 5\n",
      "2002 --> 6\n",
      "sakuranomori --> 7\n",
      "limited-edit --> 8\n",
      "morimoto --> 9\n"
     ]
    }
   ],
   "source": [
    "# view the vocabolary\n",
    "count = 0\n",
    "for key, mapped_int in vocabolary.items():\n",
    "    count +=1\n",
    "    print(key,\"-->\",mapped_int)\n",
    "    if count == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e08568-f97c-47b1-8098-7754749c03a3",
   "metadata": {},
   "source": [
    "## Search Engine v1 Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40d6c6-b30b-4d95-9a8e-6f08861766f6",
   "metadata": {},
   "source": [
    "For this type of search engine we need only to have a search engine based on the query appear or not in each documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b4c70-122c-4aa5-891e-bbaebc781b17",
   "metadata": {},
   "source": [
    "* ### Prepare the mapped document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb47079-c2de-4529-958a-ab02afd83c6d",
   "metadata": {},
   "source": [
    "To do this we will create an array of documents of each len(document_j) in which thare are converted the word into integer based on the vocabolary\n",
    "\n",
    "We will use numpy array for time optimitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "23e5b1dc-bd09-4c00-9aed-2911c22c03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that map document text to integer\n",
    "\n",
    "def word_to_int(document, vocabolary):\n",
    "    int_doc = np.zeros(len(document))\n",
    "    # iterating over the document that has len(d)<<len(vocabolary)\n",
    "    # change the value of the document, otherwise remain zero\n",
    "    for i, word in enumerate(document):\n",
    "        # vocabolary[word] is the mapping function that return an integer i.e the index\n",
    "        int_doc[i] = vocabolary[word]\n",
    "        \n",
    "    return np.sort(int_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a4281-2912-49e5-ac45-c0214c0b8732",
   "metadata": {},
   "source": [
    "* documents_mapped: is a list of list that have the words mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6b3b8b9e-cb82-4d85-a3ee-91eba7c05dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_mapped = []\n",
    "for d in documents_clean:\n",
    "    documents_mapped.append(word_to_int(d,vocabolary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fa0acee4-4694-43b1-907c-02c03e8b80e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  297.,   388.,   388.,   388.,   526.,   620.,   664.,   769.,\n",
       "         769.,   857.,   904.,  1269.,  1937.,  2145.,  2351.,  2776.,\n",
       "        2815.,  3174.,  3230.,  3232.,  3252.,  3362.,  3593.,  3738.,\n",
       "        3850.,  3881.,  3977.,  3983.,  4135.,  4161.,  4455.,  4455.,\n",
       "        4610.,  5204.,  5993.,  5993.,  6115.,  6361.,  6425.,  7445.,\n",
       "        7480.,  8026.,  8400.,  8916.,  9194.,  9580.,  9718.,  9718.,\n",
       "        9718.,  9718.,  9997., 10103., 10142., 10181., 10333., 10928.,\n",
       "       10928., 11321., 11634., 12500., 12729., 12729., 12729., 12966.,\n",
       "       13126., 13385., 13521., 13605., 13792., 13842., 13842., 13853.,\n",
       "       14284., 14444., 14758., 15035., 15073., 15078., 15218., 15768.,\n",
       "       16319., 16691., 16988., 17068., 17239., 17875., 18036., 18292.,\n",
       "       18342., 19062., 19077., 19613., 19613., 20114., 20390., 20466.,\n",
       "       20613., 21175., 21391., 21811., 22554., 22746., 23379., 24180.,\n",
       "       24623., 24843., 25170., 25587., 25723., 25798., 25898., 25903.,\n",
       "       26081.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a doc\n",
    "documents_mapped[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb014c6-1456-4524-aafb-73b8b02285c2",
   "metadata": {},
   "source": [
    "* ### Inverted Index v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6cc8d811-ed80-40e8-bcf5-7186b4bd0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "871ca101-62d6-409a-9dbd-278b31ded98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Inverted_index\n",
    "Inverted_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5415-0e3c-4771-b458-9a04c79eea53",
   "metadata": {},
   "source": [
    "To compute the Inverted Index we iterating over each document. Every time we encounter a word (that is now a integer) we insert in the dictionary the id of the documents, which is the row index in documents_mapped or in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ae9acdc8-8fd4-44a8-bfdc-0a5b06c908dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(documents_mapped):\n",
    "    for word in set(d):\n",
    "        Inverted_index[word].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fc441c2a-2e4d-4ff7-82e2-fcd1ad69b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4610.0 --> [0, 22, 36, 128, 204, 223, 331, 352, 407, 423, 483, 525, 558, 608, 641, 689, 768, 956, 1057, 1218, 1227, 1493, 1560, 1627, 1630, 1647, 1783, 1892, 2010, 2038, 2074, 2116, 2148, 2232, 2357, 2429, 2442, 2715, 2760, 2782, 3008, 3034, 3239, 3261, 3401, 3447, 3605, 3692, 3796, 3857, 3985, 4513, 4620, 4678, 4683, 4768, 4872, 4892, 4911, 4984, 5222, 5232, 5344, 5484, 5608, 5831, 6363, 6462]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key, lis in Inverted_index.items():\n",
    "    count +=1\n",
    "    print(key,\"-->\", lis)\n",
    "    if count == 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2b3f4-ae19-4dba-8eda-acd9a6fa99e3",
   "metadata": {},
   "source": [
    "Saving the Inverted Index in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4859da2e-2f6c-4fa0-9d0e-862a4c548234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open(\"Inverted_index_v1.json\", \"w\", encoding = \"utf-8\")\n",
    "json.dump(Inverted_index, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3282bc-8595-400b-acde-1f3ecb13c73e",
   "metadata": {},
   "source": [
    "* ### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1789e8-102d-4bde-933c-f0ad39de5408",
   "metadata": {},
   "source": [
    "Function for the search engine: the step by step are explain after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "93b7e12d-adc6-4565-98f9-9643f344df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_v1(query_text):\n",
    "    # pre-processing the query\n",
    "    query_clean = pre_processing(query_text)\n",
    "    query_int = word_to_int(query_clean, vocabolary)\n",
    "    \n",
    "    # finding the anime index \n",
    "    index = []\n",
    "    \n",
    "    for query in query_int:\n",
    "        # creating a set of index (set is for intersection pourpose)\n",
    "        index.append(set(Inverted_index[query]))\n",
    "    \n",
    "    index = list(index[0].intersection(*index))\n",
    "    \n",
    "    # search the url\n",
    "    with open(\"./anime_url.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    # we are searching for the anime and the url \n",
    "    anime_path = []\n",
    "    url = []\n",
    "\n",
    "    for idx in index:\n",
    "        # we need the +1 because we start indexing from 1\n",
    "        name = \"/anime_\"+str(idx+1)+\".tsv\"\n",
    "        anime_path.append(name)\n",
    "        url.append(lines[idx])\n",
    "\n",
    "    # creating the datafrae for view the result\n",
    "    animes_df = []\n",
    "    # folder of the anime_tsv\n",
    "    folder = r\"./tsv_anime/\"\n",
    "    # column I want\n",
    "    cols = [\"animeTitle\",\"animeDescription\"]\n",
    "    for i,anime_tsv in enumerate(anime_path):\n",
    "        df = pd.read_csv(folder+anime_tsv, sep = \"\\t\", usecols = cols)\n",
    "        # creating new column\n",
    "        df[\"animeURL\"] = url[i]\n",
    "        animes_df.append(df)\n",
    "    \n",
    "    frame = pd.concat(animes_df, ignore_index = True)\n",
    "    display(frame)\n",
    "    # delate the dataframe and some list to memory space\n",
    "    #del(frame)\n",
    "    #del(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "de715e2b-272c-433f-9c3b-a4ad59dadeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " alchemy alchemist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/5114/Fullmetal_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fullmetal Alchemist</td>\n",
       "      <td>Edward Elric, a young, brilliant alchemist, ha...</td>\n",
       "      <td>https://myanimelist.net/anime/121/Fullmetal_Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood Specials</td>\n",
       "      <td>Amazing secrets and startling facts are expose...</td>\n",
       "      <td>https://myanimelist.net/anime/6421/Fullmetal_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baccano!</td>\n",
       "      <td>During the early 1930s in Chicago, the transco...</td>\n",
       "      <td>https://myanimelist.net/anime/2251/Baccano\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  animeTitle  \\\n",
       "0           Fullmetal Alchemist: Brotherhood   \n",
       "1                        Fullmetal Alchemist   \n",
       "2  Fullmetal Alchemist: Brotherhood Specials   \n",
       "3                                   Baccano!   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  After a horrific alchemy experiment goes wrong...   \n",
       "1  Edward Elric, a young, brilliant alchemist, ha...   \n",
       "2  Amazing secrets and startling facts are expose...   \n",
       "3  During the early 1930s in Chicago, the transco...   \n",
       "\n",
       "                                            animeURL  \n",
       "0  https://myanimelist.net/anime/5114/Fullmetal_A...  \n",
       "1  https://myanimelist.net/anime/121/Fullmetal_Al...  \n",
       "2  https://myanimelist.net/anime/6421/Fullmetal_A...  \n",
       "3       https://myanimelist.net/anime/2251/Baccano\\n  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input query\n",
    "query_text = input()\n",
    "\n",
    "search_engine_v1(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786aee7-84c2-4cb6-bbcb-d17bececefa7",
   "metadata": {},
   "source": [
    "* ### Step by step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05cfd6-66f6-47c1-bba3-4c6edc21e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the query\n",
    "query_clean = pre_processing(query_text)\n",
    "query_int = word_to_int(query_clean, vocabolary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143b0f2-01d4-4d8b-99be-6b2c8f86bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651b2af-2317-47b0-9ea0-56eb8bcea969",
   "metadata": {},
   "source": [
    "We create a list named index that store a list of list, each one is the output from the inverted_index corrisponding to a query element. Than we intersect for obtain the documents that match ALL the query elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21b145-f0b5-453c-b563-8fe0eace0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for query in query_int:\n",
    "    index.append(set(Inverted_index[query]))\n",
    "\n",
    "index = list(index[0].intersection(*index))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9aa0e-1a94-42a8-bf92-af3f0f82cd95",
   "metadata": {},
   "source": [
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe74848-5a92-4cc8-a03a-6e2398cbcdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the url\n",
    "with open(\"./anime_url.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0e8b3-0998-4ec6-9f2f-a96a701118c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are searching for the anime and the url \n",
    "anime_path = []\n",
    "url = []\n",
    "\n",
    "for idx in index:\n",
    "    # we need the +1 because we start indexing from 1\n",
    "    name = \"/anime_\"+str(idx+1)+\".tsv\"\n",
    "    anime_path.append(name)\n",
    "    url.append(lines[idx])\n",
    "\n",
    "# creating the datafrae for view the result\n",
    "animes_df = []\n",
    "# folder of the anime_tsv\n",
    "folder = r\"./tsv_anime/\"\n",
    "# column I want\n",
    "cols = [\"animeTitle\",\"animeDescription\"]\n",
    "for i,anime_tsv in enumerate(anime_path):\n",
    "    df = pd.read_csv(folder+anime_tsv, sep = \"\\t\", usecols = cols)\n",
    "    # creating new column\n",
    "    df[\"animeURL\"] = url[i]\n",
    "    animes_df.append(df)\n",
    "    \n",
    "frame = pd.concat(animes_df, ignore_index = True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde8113-3578-40f9-a5d9-42275a5023af",
   "metadata": {},
   "source": [
    "# Searching Engine v2 Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5eace-32ea-4400-9b1c-1d4c4213a438",
   "metadata": {},
   "source": [
    "Now we want for our Inverted_index two element:\n",
    "\n",
    "* $\\text{tf}_{i,j}$: occurancy of term $j$ in document $i$\n",
    "* $\\text{idf}_{j}$: Inverse Document Frequency of term $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14d83-bfd8-47b1-8be5-f50beb47fd9d",
   "metadata": {},
   "source": [
    "Define:\n",
    "\n",
    "* n_words = total number of words in vocabolary\n",
    "* n = number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "280c1e80-bf2e-47a5-8a2a-68ad44cf82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(documents)\n",
    "n_words = len(vocabolary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa3a97-b75f-43b5-b39a-9074e22ef984",
   "metadata": {},
   "source": [
    "Creating the $\\text{tf}_{i,j}$ matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010325e-b3c7-414c-a7e3-27c6e292242f",
   "metadata": {},
   "source": [
    "* ### Prepare the mapped document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09114-ea42-403a-8236-15300fbca483",
   "metadata": {},
   "source": [
    "To do this we will create an array of documents of each len(document_j) in which thare are converted the word into integer based on the vocabolary\n",
    "\n",
    "We will use numpy array for time optimitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "68c38854-3de4-4947-9e46-ee3b5c42df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that map document text to integer\n",
    "\n",
    "def word_to_int2(document, vocabolary):\n",
    "    int_doc = np.zeros(len(vocabolary))\n",
    "    # iterating over the document that has len(d)<<len(vocabolary)\n",
    "    # change the value of the document, otherwise remain zero\n",
    "    for i, word in enumerate(document):\n",
    "        # vocabolary[word] is the mapping function that return an integer i.e the index\n",
    "        int_doc[int(vocabolary[word])] += 1\n",
    "        \n",
    "    return int_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf34732-bd64-491d-ac60-c1e59eb1784c",
   "metadata": {},
   "source": [
    "* documents_mapped: is a list of list that have the words mapped with is count é I have the tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f4d2dd79-2c27-4daa-a30b-58283b95fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_mappedv2 = []\n",
    "for d in documents_clean:\n",
    "    documents_mappedv2.append(word_to_int2(d,vocabolary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "38cbc6c3-5794-4bba-af1c-bd190c886ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 4., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       3., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a doc\n",
    "documents_mappedv2[0][documents_mappedv2[0]>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dcb25f-f78c-4a8a-b596-60cac85c86de",
   "metadata": {},
   "source": [
    "Creating the $\\text{idf}_{j}$ array:\n",
    "* I need the $n_j$: number of documents containing term j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "62f831d7-4b8b-4f8a-b8ec-47693f74a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nj = np.zeros(n_words, dtype = np.int64)\n",
    "\n",
    "for d in documents_mapped:\n",
    "    for word in set(d):\n",
    "        nj[int(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d5bd0c85-3665-4cbb-9fac-aa9cf20f0c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 14, ..., 30,  3,  1], dtype=int64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view\n",
    "nj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c709c9f8-4245-4ac7-a6d0-913692b7c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = documents_mappedv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0e12b027-5cbf-4cfc-adf7-52f828b44c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating idf_j\n",
    "\n",
    "idf = np.zeros(n_words, dtype = np.float64)\n",
    "idf = np.log(n/nj) / np.log(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4bc0a88d-f2d5-4b70-bce9-81dd6180f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdf = np.multiply(tf,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "68334ae5-0238-4679-993e-4da30ab77c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 26282)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97126690-caf1-4ab9-b1d2-8830593aa4fe",
   "metadata": {},
   "source": [
    "* ### Inverted indexi v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bf717bc4-d645-4c30-bceb-5563a9ccde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "320fbce1-4a8c-4421-90c9-55d7b541a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inverted_indexv2 = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1e876-00b4-4667-8d9c-97ae4126dc8a",
   "metadata": {},
   "source": [
    "To compute the Inverted Index we iterating over each document. Every time we encounter a word (that is now a integer) we insert in the dictionary the __tupla__ of _id_ of the documents and the _tfIdf_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fffbd38e-ab21-405d-ac34-9129956e6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(documents_mapped):\n",
    "    for word in set(d):\n",
    "        Inverted_indexv2[int(word)].append((i,tfIdf[i,int(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "849981a2-144c-4e1b-a02f-031b6f7c48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4610 --> [(0, 0.5193940325148905), (22, 0.5193940325148905), (36, 0.5193940325148905), (128, 0.5193940325148905), (204, 0.5193940325148905), (223, 0.5193940325148905), (331, 0.5193940325148905), (352, 0.5193940325148905), (407, 0.5193940325148905), (423, 0.5193940325148905), (483, 0.5193940325148905), (525, 0.5193940325148905), (558, 0.5193940325148905), (608, 0.5193940325148905), (641, 0.5193940325148905), (689, 0.5193940325148905), (768, 0.5193940325148905), (956, 0.5193940325148905), (1057, 0.5193940325148905), (1218, 0.5193940325148905), (1227, 0.5193940325148905), (1493, 1.038788065029781), (1560, 0.5193940325148905), (1627, 1.038788065029781), (1630, 0.5193940325148905), (1647, 0.5193940325148905), (1783, 0.5193940325148905), (1892, 0.5193940325148905), (2010, 0.5193940325148905), (2038, 0.5193940325148905), (2074, 0.5193940325148905), (2116, 0.5193940325148905), (2148, 0.5193940325148905), (2232, 0.5193940325148905), (2357, 0.5193940325148905), (2429, 0.5193940325148905), (2442, 0.5193940325148905), (2715, 1.038788065029781), (2760, 0.5193940325148905), (2782, 0.5193940325148905), (3008, 0.5193940325148905), (3034, 1.038788065029781), (3239, 0.5193940325148905), (3261, 0.5193940325148905), (3401, 0.5193940325148905), (3447, 0.5193940325148905), (3605, 0.5193940325148905), (3692, 0.5193940325148905), (3796, 0.5193940325148905), (3857, 0.5193940325148905), (3985, 0.5193940325148905), (4513, 1.038788065029781), (4620, 0.5193940325148905), (4678, 0.5193940325148905), (4683, 0.5193940325148905), (4768, 0.5193940325148905), (4872, 0.5193940325148905), (4892, 0.5193940325148905), (4911, 0.5193940325148905), (4984, 0.5193940325148905), (5222, 0.5193940325148905), (5232, 0.5193940325148905), (5344, 0.5193940325148905), (5484, 0.5193940325148905), (5608, 0.5193940325148905), (5831, 0.5193940325148905), (6363, 0.5193940325148905), (6462, 0.5193940325148905)]\n"
     ]
    }
   ],
   "source": [
    "# view\n",
    "count = 0\n",
    "for key, lis in Inverted_indexv2.items():\n",
    "    count +=1\n",
    "    print(key,\"-->\",lis)\n",
    "    if count == 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf89239-e185-4cc8-98ef-861c27856db8",
   "metadata": {},
   "source": [
    "Saving the Inverted Index in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "21dd4b0d-0feb-48c0-84f7-9a825b6d7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open(\"Inverted_index_v2.json\", \"w\")\n",
    "json.dump(Inverted_indexv2, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0ed56-c4dc-483e-bd55-5b15db9a6d1b",
   "metadata": {},
   "source": [
    "* ### Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "cf0edd28-7ddb-4caf-ae01-2b559fa5f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " alchemy alchemist\n"
     ]
    }
   ],
   "source": [
    "# input query\n",
    "query_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "85254e6f-4ae0-4873-8589-df6e79446cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_clean = pre_processing(query_text)\n",
    "query_int = word_to_int2(query_clean, vocabolary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "af09c9de-0318-4c78-b153-3dbdeabe662c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689824f-847c-4e58-bafd-d539992cfd79",
   "metadata": {},
   "source": [
    "We define the cosine similarity as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\cos(q,d^i) =  \\frac{\\sum_{j=1}^d q_j*d_{ij}}{||d^i||*||q||}\n",
    "\\end{equation}\n",
    "$$\n",
    "Recall: $d^i = [\\text{tfIdf}_{i1}, \\text{tfIdf}_{i2}, \\ldots]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "0d6585f6-698a-4df9-a3f5-c87fa9829933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a,b):\n",
    "    cosine = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "9d3a033a-3563-4392-a96d-5a857a6082f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5993, 16691], dtype=int64)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(query_int>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af841b87-ad92-4efa-9282-f1a7c2a24383",
   "metadata": {},
   "source": [
    "Create the list of each match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "c0ac9000-dfbf-4eee-a4ed-76576d29875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_list = []\n",
    "lenMatch = []\n",
    "# tupla = (#list, len(list))\n",
    "max_lenMatch = (0,-1)\n",
    "for i,query in enumerate(np.where(query_int>0)[0]):\n",
    "    lis = Inverted_indexv2[int(query)]\n",
    "    match_list.append(lis)\n",
    "    tmplis, tmplen = i, len(lis)\n",
    "    if tmplen>max_lenMatch[1]:\n",
    "        max_lenMatch = (tmplis,tmplen)\n",
    "    lenMatch.append(len(lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "30c9f42d-254a-4449-82b7-7b8bfda6c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum(lists, endlist):\n",
    "    # get the minimum and the argmin\n",
    "    #idd = [idd[0] for idd in lists]\n",
    "    lis = []\n",
    "    for i,idd in enumerate(lists):\n",
    "        # be care to take the minimum for only the \"still running\" pointer\n",
    "        if i in np.where(endlist == 0)[0]:\n",
    "            lis.append(idd[0])\n",
    "    minimum = min(lis)\n",
    "    \n",
    "    return minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "1d2ce172-2061-41bf-8bc2-16bcc5950af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_pointer(match_list, pointer, minimum, lenMatch, endlist):\n",
    "    for i in range(len(pointer)):\n",
    "        if  pointer[i] == lenMatch[i]-1:\n",
    "            # endlist is a list of 0 and 1\n",
    "            # 0 if the list is still running, 1 if finished\n",
    "            endlist[i] = 1\n",
    "        # increase the pointer if I score the min BUT take care of len\n",
    "        target = match_list[i][0]\n",
    "        if  target == minimum and pointer[i] < lenMatch[i]-1:\n",
    "            pointer[i] += 1\n",
    "    return pointer, endlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "a2822b74-8508-4839-91ff-4b070ec9fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching engine:\n",
    "\n",
    "# initialize \n",
    "m = len(query_clean)\n",
    "pointer = np.zeros(m, dtype = \"int\")\n",
    "scores = []\n",
    "endlist = np.zeros(m, dtype = \"int\")\n",
    "# while loop untile escape the maximum list: I want all the score at least 1 match\n",
    "end = True\n",
    "while(end):\n",
    "    # get the list of element pointed by the pointer\n",
    "    lis = []\n",
    "    for i in range(m):\n",
    "        lis.append(match_list[i][pointer[i]])\n",
    "    # get the minimum and the argmin\n",
    "    mini = minimum(lis,endlist)\n",
    "    # compute the score:\n",
    "    score = cosine_sim(query_int, tfIdf[mini])\n",
    "    # heappush will heap by first element\n",
    "    # !!! -score because I heap sort for min value\n",
    "    heapq.heappush(scores,(-score, mini))\n",
    "    # increase the pointer\n",
    "    pointer, endlist = increase_pointer(lis, pointer, mini, lenMatch, endlist)  \n",
    "    # when finishing the list, escape\n",
    "    if endlist.all() == 1:\n",
    "        end = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "f87f8bb1-3362-418d-aaa8-3e1c443702d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "topscore = []\n",
    "topk = []\n",
    "for i in range(k):\n",
    "    topscoretmp, topktmp = heapq.heappop(scores)\n",
    "    topscore.append(abs(topscoretmp)), topk.append(topktmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "d43850f2-59c7-4155-a1e0-8f3b4039cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeURL</th>\n",
       "      <th>animeScores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arcana Famiglia: Capriccio - stile Arcana Fami...</td>\n",
       "      <td>After toiling away in his lab, the alchemist J...</td>\n",
       "      <td>https://myanimelist.net/anime/15411/Arcana_Fam...</td>\n",
       "      <td>0.114412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fullmetal Alchemist: Premium Collection</td>\n",
       "      <td>1. State Alchemists vs Seven Homunculi\\n\\nA 10...</td>\n",
       "      <td>https://myanimelist.net/anime/908/Fullmetal_Al...</td>\n",
       "      <td>0.097918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birthday Wonderland</td>\n",
       "      <td>The day before her birthday, Akane is asked to...</td>\n",
       "      <td>https://myanimelist.net/anime/38985/Birthday_W...</td>\n",
       "      <td>0.086791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gosick</td>\n",
       "      <td>Kazuya Kujou is a foreign student at Saint Mar...</td>\n",
       "      <td>https://myanimelist.net/anime/8425/Gosick\\n</td>\n",
       "      <td>0.081709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trinity Seven Movie 1: Eternity Library to Alc...</td>\n",
       "      <td>The film's story begins when Arata inadvertent...</td>\n",
       "      <td>https://myanimelist.net/anime/33581/Trinity_Se...</td>\n",
       "      <td>0.081366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senki Zesshou Symphogear GX</td>\n",
       "      <td>Following the events of Senki Zesshou Symphoge...</td>\n",
       "      <td>https://myanimelist.net/anime/21573/Senki_Zess...</td>\n",
       "      <td>0.078723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fullmetal Alchemist: The Sacred Star of Milos</td>\n",
       "      <td>Chasing a runaway alchemist with strange power...</td>\n",
       "      <td>https://myanimelist.net/anime/9135/Fullmetal_A...</td>\n",
       "      <td>0.076657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Garo: Vanishing Line</td>\n",
       "      <td>Corruption looms over the prosperous Russell C...</td>\n",
       "      <td>https://myanimelist.net/anime/36144/Garo__Vani...</td>\n",
       "      <td>0.071497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senki Zesshou Symphogear AXZ</td>\n",
       "      <td>Hibiki Tachibana has defeated many powerful en...</td>\n",
       "      <td>https://myanimelist.net/anime/32836/Senki_Zess...</td>\n",
       "      <td>0.063786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Garo: Honoo no Kokuin</td>\n",
       "      <td>In the name of the king, the Valiante Kingdom ...</td>\n",
       "      <td>https://myanimelist.net/anime/23311/Garo__Hono...</td>\n",
       "      <td>0.057814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "0  Arcana Famiglia: Capriccio - stile Arcana Fami...   \n",
       "1            Fullmetal Alchemist: Premium Collection   \n",
       "2                                Birthday Wonderland   \n",
       "3                                             Gosick   \n",
       "4  Trinity Seven Movie 1: Eternity Library to Alc...   \n",
       "5                        Senki Zesshou Symphogear GX   \n",
       "6      Fullmetal Alchemist: The Sacred Star of Milos   \n",
       "7                               Garo: Vanishing Line   \n",
       "8                       Senki Zesshou Symphogear AXZ   \n",
       "9                              Garo: Honoo no Kokuin   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  After toiling away in his lab, the alchemist J...   \n",
       "1  1. State Alchemists vs Seven Homunculi\\n\\nA 10...   \n",
       "2  The day before her birthday, Akane is asked to...   \n",
       "3  Kazuya Kujou is a foreign student at Saint Mar...   \n",
       "4  The film's story begins when Arata inadvertent...   \n",
       "5  Following the events of Senki Zesshou Symphoge...   \n",
       "6  Chasing a runaway alchemist with strange power...   \n",
       "7  Corruption looms over the prosperous Russell C...   \n",
       "8  Hibiki Tachibana has defeated many powerful en...   \n",
       "9  In the name of the king, the Valiante Kingdom ...   \n",
       "\n",
       "                                            animeURL  animeScores  \n",
       "0  https://myanimelist.net/anime/15411/Arcana_Fam...     0.114412  \n",
       "1  https://myanimelist.net/anime/908/Fullmetal_Al...     0.097918  \n",
       "2  https://myanimelist.net/anime/38985/Birthday_W...     0.086791  \n",
       "3        https://myanimelist.net/anime/8425/Gosick\\n     0.081709  \n",
       "4  https://myanimelist.net/anime/33581/Trinity_Se...     0.081366  \n",
       "5  https://myanimelist.net/anime/21573/Senki_Zess...     0.078723  \n",
       "6  https://myanimelist.net/anime/9135/Fullmetal_A...     0.076657  \n",
       "7  https://myanimelist.net/anime/36144/Garo__Vani...     0.071497  \n",
       "8  https://myanimelist.net/anime/32836/Senki_Zess...     0.063786  \n",
       "9  https://myanimelist.net/anime/23311/Garo__Hono...     0.057814  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # search the url\n",
    "with open(\"./anime_url.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "        \n",
    "# we are searching for the anime and the url \n",
    "anime_path = []\n",
    "url = []\n",
    "\n",
    "for idx in topk:\n",
    "    # we need the +1 because we start indexing from 1\n",
    "    name = \"/anime_\"+str(idx+1)+\".tsv\"\n",
    "    anime_path.append(name)\n",
    "    url.append(lines[idx])\n",
    "\n",
    "# creating the datafrae for view the result\n",
    "animes_df = []\n",
    "# folder of the anime_tsv\n",
    "folder = r\"./tsv_anime/\"\n",
    "# column I want\n",
    "cols = [\"animeTitle\",\"animeDescription\"]\n",
    "for i,anime_tsv in enumerate(anime_path):\n",
    "    df = pd.read_csv(folder+anime_tsv, sep = \"\\t\", usecols = cols)\n",
    "    # creating new column\n",
    "    df[\"animeURL\"] = url[i]\n",
    "    df[\"animeScores\"] = topscore[i]\n",
    "    animes_df.append(df)\n",
    "    \n",
    "frame = pd.concat(animes_df, ignore_index = True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79bc16-66c3-4a67-b4d9-7c3df4d3a396",
   "metadata": {},
   "source": [
    "# Cheak intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "fd8c7376-c7b2-4f3a-a049-c31d903411e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = []\n",
    "for x in match_list:\n",
    "    tmp = []\n",
    "    for tupla in x:\n",
    "        tmp.append((tupla[0]))\n",
    "    index1.append(set(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "d2bf6491-60b3-42b2-b02c-7c278ebdad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexxx = list(index1[0].intersection(*index1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "e2b9de96-fe8d-46a4-8e93-2768d3ecdf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[525, 167]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "f5c15b79-058b-4449-9b7b-8f444e2e94ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6270, 393, 525, 1490, 167, 0, 2184, 2420, 4155, 4465]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27941da4-57bf-4aa4-aab8-c3d1cee4cf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
